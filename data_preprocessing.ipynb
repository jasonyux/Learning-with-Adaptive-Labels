{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "\t\"bert-base-uncased\",\n",
    "\tpadding=True, padding_side='left', truncation=True, truncation_side='left'\n",
    ")\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_bert_input(x:dict, batch_size):\n",
    "\tbatched_input = []\n",
    "\tnum_samples = len(x['input_ids'])\n",
    "\tfor i in range(0, num_samples, batch_size):\n",
    "\t\tend_i = min(i + batch_size, num_samples)\n",
    "\t\tbatched_input.append({k: v[i:end_i] for k, v in x.items()})\n",
    "\treturn batched_input\n",
    "\n",
    "\n",
    "class FOOD101_Loader(object):\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef load_data():\n",
    "\t\tds_train, ds_test = tfds.load('food101', split=['train', 'validation'], shuffle_files=False, as_supervised=True)\n",
    "\t\t\n",
    "\t\t# since food101 images are big, we resize them to 32x32\n",
    "\t\tx_train = []\n",
    "\t\ty_train = []\n",
    "\n",
    "\t\tfor image, label in tqdm(tfds.as_numpy(ds_train)):\n",
    "\t\t\tresized = cv2.resize(image, (32, 32), interpolation=cv2.INTER_AREA)\n",
    "\t\t\tx_train.append(resized)\n",
    "\t\t\ty_train.append(label)\n",
    "\t\tx_train = np.stack(x_train)\n",
    "\t\ty_train = np.stack(y_train)\n",
    "\n",
    "\t\tx_test = []\n",
    "\t\ty_test = []\n",
    "\n",
    "\t\tfor image, label in tqdm(tfds.as_numpy(ds_test)):\n",
    "\t\t\tresized = cv2.resize(image, (32, 32), interpolation=cv2.INTER_AREA)\n",
    "\t\t\tx_test.append(resized)\n",
    "\t\t\ty_test.append(label)\n",
    "\t\tx_test = np.stack(x_test)\n",
    "\t\ty_test = np.stack(y_test)\n",
    "\t\treturn (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "class Text_Dataset_Loader(object):\n",
    "\tdef __init__(self, dset_name, num_classes):\n",
    "\t\tself.dset_name = dset_name\n",
    "\t\tself.num_classes = num_classes\n",
    "\n",
    "\tdef load_data(self):\n",
    "\t\tds_train, ds_test = tfds.load(self.dset_name, split=['train', 'test'], shuffle_files=False, as_supervised=True)\n",
    "\t\t\n",
    "\t\t# encode the text\n",
    "\t\tx_train_text = []\n",
    "\t\ty_train = []\n",
    "\t\tfor text, label in tfds.as_numpy(ds_train):\n",
    "\t\t\tx_train_text.append(text.decode('utf-8'))\n",
    "\t\t\ty_train.append(label)\n",
    "\n",
    "\t\tx_test_text = []\n",
    "\t\ty_test = []\n",
    "\t\tfor text, label in tfds.as_numpy(ds_test):\n",
    "\t\t\tx_test_text.append(text.decode('utf-8'))\n",
    "\t\t\ty_test.append(label)\n",
    "\n",
    "\t\tmax_length = 200 if self.dset_name == 'imdb_reviews' else 100\n",
    "\t\tprint(\"tokenizing text. This might take a while...\")\n",
    "\t\tx_train_text_encoded = tokenizer(x_train_text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"np\")\n",
    "\t\tx_test_text_encoded = tokenizer(x_test_text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"np\")\n",
    "\n",
    "\t\t# batch the train and test dset for BERT encoding\n",
    "\t\tx_train_text_encoded_batched = batched_bert_input(x_train_text_encoded, batch_size=64)\n",
    "\t\tx_test_text_encoded_batched = batched_bert_input(x_test_text_encoded, batch_size=64)\n",
    "\n",
    "\t\t# perform BERT embedding\n",
    "\t\tx_train_embeddings = []\n",
    "\t\tprint(\"encoding train text\")\n",
    "\t\tfor batch in tqdm(x_train_text_encoded_batched):\n",
    "\t\t\tx_train_embeddings.append(bert_model(**batch).pooler_output)\n",
    "\n",
    "\t\tx_test_embeddings = []\n",
    "\t\tprint(\"encoding test text\")\n",
    "\t\tfor batch in tqdm(x_test_text_encoded_batched):\n",
    "\t\t\tx_test_embeddings.append(bert_model(**batch).pooler_output)\n",
    "\n",
    "\t\tx_train_embeddings = tf.concat(x_train_embeddings, axis=0)\n",
    "\t\tx_test_embeddings = tf.concat(x_test_embeddings, axis=0)\n",
    "\n",
    "\t\t# output\n",
    "\t\tx_train = x_train_embeddings\n",
    "\t\ty_train = tf.constant(y_train)\n",
    "\t\ty_train = tf.one_hot(y_train, depth=self.num_classes)\n",
    "\n",
    "\t\tx_test = x_test_embeddings\n",
    "\t\ty_test = tf.constant(y_test)\n",
    "\t\ty_test = tf.one_hot(y_test, depth=self.num_classes)\n",
    "\t\treturn (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "class AwA2_Precomputed(object):\n",
    "\tdef __init__(self):\n",
    "\t\t# first download the awa2 dataset to this directory \"datasets/raw/awa2\"\n",
    "\t\tself.feature_path = f\"datasets/raw/awa2/precomputed/AwA2-features.txt\"\n",
    "\t\tself.label_path = f\"datasets/raw/awa2/precomputed/AwA2-labels.txt\"\n",
    "\n",
    "\t\tclassname_file = f\"datasets/raw/awa2/classes.txt\"\n",
    "\t\tidx_to_classname = {}\n",
    "\t\twith open(classname_file, \"rt\") as f:\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\tidx, classname = line.strip().split(\"\\t\")\n",
    "\t\t\t\tidx_to_classname[int(idx)-1] = classname\n",
    "\t\tself.idx_to_classname = idx_to_classname\n",
    "\n",
    "\tdef __read_txt(self, path):\n",
    "\t\twith open(path, \"rt\") as f:\n",
    "\t\t\tout = np.loadtxt(f, delimiter=\" \")\n",
    "\t\treturn out\n",
    "\n",
    "\tdef load_all(self):\n",
    "\t\tX = self.__read_txt(self.feature_path)\n",
    "\t\tY = self.__read_txt(self.label_path)\n",
    "\t\tY = Y.astype(np.int64) - 1 # so we start with 0\n",
    "\t\treturn X, Y\n",
    "\n",
    "\n",
    "class AwA2_Normal_Precomputed(AwA2_Precomputed):\n",
    "\t\"\"\"AwA2 dataset with a Normal train/test split on precomputed features.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# first download the awa2 dataset to this directory \"datasets/raw/awa2\"\n",
    "\t\tself.split_idx_file = f\"datasets/raw/awa2/precomputed/train_test_idx.npz\"\n",
    "\t\t\n",
    "\t\tsplit_idx = np.load(self.split_idx_file)\n",
    "\t\tself.X_train_idx = split_idx[\"train_idx\"]\n",
    "\t\tself.X_test_idx = split_idx[\"test_idx\"]\n",
    "\t\n",
    "\tdef load_data(self):\n",
    "\t\tX, Y = self.load_all()\n",
    "\t\tX_train = X[self.X_train_idx]\n",
    "\t\tX_test = X[self.X_test_idx]\n",
    "\t\tY_train = Y[self.X_train_idx]\n",
    "\t\tY_test = Y[self.X_test_idx]\n",
    "\t\treturn (X_train, Y_train), (X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_TO_DSET = {\n",
    "\t\"mnist\": tf.keras.datasets.mnist,\n",
    "\t\"fashion_mnist\": tf.keras.datasets.fashion_mnist,\n",
    "\t\"cifar10\": tf.keras.datasets.cifar10,\n",
    "\t\"cifar100\": tf.keras.datasets.cifar100,\n",
    "\t\"food101\": FOOD101_Loader(),\n",
    "\t\"awa2_n_precomputed\": AwA2_Normal_Precomputed(), # this requires first munally downloading the AwA2 dataset\n",
    "\t\"imdb_reviews\": Text_Dataset_Loader('imdb_reviews', 2),\n",
    "\t\"yelp_polarity_reviews\": Text_Dataset_Loader('yelp_polarity_reviews', 2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dset_name, num_classes, is_image_dset=True):\n",
    "\t(x_train, y_train), (x_test, y_test) = NAME_TO_DSET[dset_name].load_data()\n",
    "\n",
    "\tif is_image_dset:\n",
    "\t\ty_train = y_train.reshape(-1)\n",
    "\t\tx_train = x_train.astype(\"float32\")/ 255.\n",
    "\t\ty_train = tf.one_hot(y_train, depth=num_classes)\n",
    "\n",
    "\t\ty_test = y_test.reshape(-1)\n",
    "\t\tx_test = x_test.astype(\"float32\")/ 255.\n",
    "\t\ty_test = tf.one_hot(y_test, depth=num_classes)\n",
    "\treturn x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def save(dset_name, num_classes, x_train, y_train, x_test, y_test):\n",
    "\ttrain_prefix = f\"{num_classes}_cls_all_percls\"\n",
    "\ttest_prefix = f\"{num_classes}_cls_all_percls\"\n",
    "\n",
    "\ttrain_save_path = f\"datasets/{dset_name}/train/{train_prefix}.npz\"\n",
    "\tprint(f\"Saving train data to {train_save_path}\")\n",
    "\t# save train dset\n",
    "\tif not os.path.exists(f\"datasets/{dset_name}/train\"):\n",
    "\t\tos.makedirs(f\"datasets/{dset_name}/train\")\n",
    "\tnp.savez(train_save_path, x=x_train, y=y_train) # compressed\n",
    "\n",
    "\ttest_save_path = f\"datasets/{dset_name}/test/{test_prefix}.npz\"\n",
    "\tprint(f\"Saving test data to {test_save_path}\")\n",
    "\t# save test dset\n",
    "\tif not os.path.exists(f\"datasets/{dset_name}/test\"):\n",
    "\t\tos.makedirs(f\"datasets/{dset_name}/test\")\n",
    "\tnp.savez(test_save_path, x=x_test, y=y_test)\n",
    "\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train data to datasets/cifar10/train/10_cls_all_percls.npz\n",
      "Saving test data to datasets/cifar10/test/10_cls_all_percls.npz\n"
     ]
    }
   ],
   "source": [
    "# e.g.\n",
    "dset_name = \"cifar10\"\n",
    "num_classes = 10\n",
    "x_train, y_train, x_test, y_test = process_data(dset_name, num_classes, is_image_dset=True)\n",
    "save(dset_name, num_classes, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing text\n",
      "encoding train text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdc35f73d0447309ab7539691146236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding test text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcc6c8fdc0b4cfab4cfdf98a74bdc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train data to datasets/imdb_reviews/train/2_cls_all_percls.npz\n",
      "Saving test data to datasets/imdb_reviews/test/2_cls_all_percls.npz\n"
     ]
    }
   ],
   "source": [
    "# e.g.\n",
    "dset_name = \"imdb_reviews\"\n",
    "num_classes = 2\n",
    "x_train, y_train, x_test, y_test = process_data(dset_name, num_classes, is_image_dset=False)\n",
    "save(dset_name, num_classes, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
